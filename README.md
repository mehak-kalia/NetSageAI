
# ğŸ§  NetSageAI

NetSageAI is a Generative AI-powered assistant for network engineers. It diagnoses network issues from router logs and generates vendor-specific CLI configurations using open-source LLMs like `llama3` via [Ollama](https://ollama.com).

<p align="center">
  <img src="https://img.shields.io/badge/Built%20With-FastAPI%20%7C%20React%20%7C%20Ollama-0A0A0A?style=flat&logo=vercel&logoColor=white" alt="stack badge" />
  <img src="https://img.shields.io/badge/License-MIT-blue.svg" alt="license badge" />
</p>

---

<pre lang="md"> ## ğŸ–¼ï¸ Screenshot Hereâ€™s a preview of the **NetSageAI** web app: ![NetSageAI Screenshot](assets/ChatGPT Image Jun 21, 2025, 05_34_00 PM.png) </pre>

## âœ¨ Features

- ğŸ“„ Upload router log files and receive AI-powered diagnosis  
- âš™ï¸ Generate vendor-style CLI configurations based on your intent  
- ğŸ§  Uses local open-source LLMs (`llama3`, `mistral`) via Ollama  
- ğŸ§© FastAPI backend and React frontend with Tailwind styling  
- ğŸ” Fully offline and private â€” no OpenAI API required  

---

## ğŸ“ Project Structure

```
NetSageAI/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                  # FastAPI entrypoint
â”‚   â”œâ”€â”€ ollama_client.py         # LLM integration via /api/chat
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ analyze.py           # /analyze endpoint logic
â”‚   â”‚   â””â”€â”€ generate_config.py   # /generate-config endpoint
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ sample_logs/         # Sample router logs
â”œâ”€â”€ frontend/                    # React + Tailwind UI
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Backend Setup

#### ğŸ§° Requirements

- Python 3.9+
- Ollama installed (`brew install ollama`)
- LLM pulled locally (e.g. `llama3`)

#### ğŸ”§ Instructions

```bash
cd backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# In another terminal
ollama serve            # Start Ollama server
ollama pull llama3      # Pull the LLM

# Back in backend terminal
uvicorn main:app --reload
```

> FastAPI will be running at: `http://localhost:8000`

---

### 2ï¸âƒ£ Frontend Setup

#### ğŸ§° Requirements

- Node.js 18+ (recommended via `nvm` or `brew install node`)

#### ğŸ”§ Instructions

```bash
cd frontend
npm install
npm run dev
```

> Frontend runs at: `http://localhost:5173`

---

## ğŸ§ª API Reference

### ğŸ” POST `/analyze`

Analyze uploaded network log file and return possible issues.

#### Request:
```bash
POST /analyze
Content-Type: multipart/form-data
file: <router_log.txt>
```

#### Response:
```json
{
  "diagnosis": "OSPF neighbor down event detected on Fa0/1 due to interface flap."
}
```

---

### âš™ï¸ POST `/generate-config`

Generate vendor-style CLI configuration from user intent.

#### Request:
```bash
POST /generate-config
Content-Type: multipart/form-data
vendor: Cisco
intent: Configure OSPF on interface Gi0/1 in area 0
```

#### Response:
```json
{
  "config": "interface Gi0/1\n ip ospf 1 area 0\n ip address ..."
}
```

---

## ğŸ”’ Privacy First

All data is processed locally â€” no external API calls. This project runs 100% offline using models served by Ollama.

---

## ğŸ“Œ Roadmap

- [x] Log file diagnosis via LLM
- [x] Vendor-specific config generation
- [ ] Chat memory support
- [ ] Model selector UI (llama3, mistral, etc.)
- [ ] Docker container for fullstack setup
- [ ] Export config as `.txt` or `.conf` file

---

## ğŸ§‘â€ğŸ’» Author

**Mehak Kalia**  
ğŸ’¼ Generative AI & Networking Projects  
ğŸ“« [LinkedIn](https://www.linkedin.com/in/mehak-kalia) | ğŸ› ï¸ [GitHub](https://github.com/mehakkalia)

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).
```
